{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"SKNahin/bengali-transliteration-data\")\nprint(ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:02:54.603384Z","iopub.execute_input":"2024-12-21T14:02:54.603676Z","iopub.status.idle":"2024-12-21T14:02:57.929088Z","shell.execute_reply.started":"2024-12-21T14:02:54.603638Z","shell.execute_reply":"2024-12-21T14:02:57.928326Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/300 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc1ceb3c9f434723b07f541fc8e1267b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/333k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"285b8a38e7414e9cb1222ea8931a94ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5006 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f0863dab9d437a8e10769c31b922db"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['bn', 'rm'],\n        num_rows: 5006\n    })\n})\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"split = ds['train'].train_test_split(test_size=0.2,seed=42)\nds_train = split['train']\nds_val = split['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:07:05.723412Z","iopub.execute_input":"2024-12-21T14:07:05.723864Z","iopub.status.idle":"2024-12-21T14:07:05.739887Z","shell.execute_reply.started":"2024-12-21T14:07:05.723822Z","shell.execute_reply":"2024-12-21T14:07:05.738887Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:03:09.530503Z","iopub.execute_input":"2024-12-21T14:03:09.530781Z","iopub.status.idle":"2024-12-21T14:03:13.839801Z","shell.execute_reply.started":"2024-12-21T14:03:09.530760Z","shell.execute_reply":"2024-12-21T14:03:13.838797Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small')\ntokenizer = T5Tokenizer.from_pretrained('t5-small',use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:09:37.152530Z","iopub.execute_input":"2024-12-21T14:09:37.152816Z","iopub.status.idle":"2024-12-21T14:09:37.996206Z","shell.execute_reply.started":"2024-12-21T14:09:37.152794Z","shell.execute_reply":"2024-12-21T14:09:37.995520Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Model Selection\nWe have used t5-small. Because it is multilingual and mBART doesn't support bengali\nT5 is efficient and we have used T5-small for faster training","metadata":{}},{"cell_type":"code","source":"def preprocess(ds):\n    def tokenize_func(ds1):\n        inputs = ds1[\"rm\"]\n        outputs = ds1[\"bn\"]\n        model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=128)\n        labels = tokenizer(outputs, padding=\"max_length\", truncation=True, max_length=128)\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n    ds = ds.map(tokenize_func, batched=True)\n    ds = ds.filter(\n        lambda x: 1 <= len(x[\"rm\"]) <= 120 and 1 <= len(x[\"bn\"]) <= 120\n    )\n    return ds\nds_train = preprocess(ds_train)\nds_val = preprocess(ds_val)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:13:04.134193Z","iopub.execute_input":"2024-12-21T14:13:04.134504Z","iopub.status.idle":"2024-12-21T14:13:05.967194Z","shell.execute_reply.started":"2024-12-21T14:13:04.134482Z","shell.execute_reply":"2024-12-21T14:13:05.966457Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3938 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f2059acb0e443ff83b4444153636894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3938 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c3555dd8b2440338d54aeded2505f70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/979 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f2f865e28f2450aa7a1dbb1985f4c40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/979 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61b05fb6ad04cbaaaafd62adcac907c"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/results',            \n    evaluation_strategy=\"epoch\",                     \n    learning_rate=5e-5,                              \n    per_device_train_batch_size=8,                  \n    per_device_eval_batch_size=8,                   \n    num_train_epochs=1,                              \n    weight_decay=0.01,                               \n    logging_dir='/kaggle/working/logs',              \n    save_strategy=\"epoch\",                           \n    load_best_model_at_end=True,                     \n    metric_for_best_model=\"loss\",                    \n    gradient_accumulation_steps=2,                   \n    fp16=True,                                       \n    eval_steps=100,                                  \n    save_steps=500,                                  \n    push_to_hub=False,                               \n    report_to=\"none\",                                \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:13:13.714457Z","iopub.execute_input":"2024-12-21T14:13:13.714774Z","iopub.status.idle":"2024-12-21T14:13:13.749077Z","shell.execute_reply.started":"2024-12-21T14:13:13.714748Z","shell.execute_reply":"2024-12-21T14:13:13.748292Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Hyperparameter\n1. Learning Rate: We used 5e-5 for faster convergence\n2. batch_size: We initially wanted to use 32 but kaggle couldn't afford it so we had to settle for 8\n3. fp16 mixed precision: For faster and less memory\n4. number of epochs: Have used 1 for quick results","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,                         \n    args=training_args,                  \n    train_dataset=ds_train,              \n    eval_dataset=ds_val,                 \n    tokenizer=tokenizer,                 \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:13:19.532195Z","iopub.execute_input":"2024-12-21T14:13:19.532491Z","iopub.status.idle":"2024-12-21T14:13:19.544494Z","shell.execute_reply.started":"2024-12-21T14:13:19.532468Z","shell.execute_reply":"2024-12-21T14:13:19.543660Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:13:22.474152Z","iopub.execute_input":"2024-12-21T14:13:22.474501Z","iopub.status.idle":"2024-12-21T14:14:23.503270Z","shell.execute_reply.started":"2024-12-21T14:13:22.474472Z","shell.execute_reply":"2024-12-21T14:14:23.502456Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [123/123 00:59, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>0.198911</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=123, training_loss=1.8238436070884145, metrics={'train_runtime': 60.1665, 'train_samples_per_second': 65.452, 'train_steps_per_second': 2.044, 'total_flos': 133176332648448.0, 'train_loss': 1.8238436070884145, 'epoch': 0.9959514170040485})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"results = trainer.evaluate()\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:15:07.702708Z","iopub.execute_input":"2024-12-21T14:15:07.703047Z","iopub.status.idle":"2024-12-21T14:15:13.101026Z","shell.execute_reply.started":"2024-12-21T14:15:07.703010Z","shell.execute_reply":"2024-12-21T14:15:13.100131Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [62/62 00:05]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.19891144335269928, 'eval_runtime': 5.39, 'eval_samples_per_second': 181.633, 'eval_steps_per_second': 11.503, 'epoch': 0.9959514170040485}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"trainer.save_model('/kaggle/working/bengali_transliteration_model')\ntokenizer.save_pretrained('/kaggle/working/bengali_transliteration_model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:32:37.524217Z","iopub.execute_input":"2024-12-21T14:32:37.524564Z","iopub.status.idle":"2024-12-21T14:32:38.106743Z","shell.execute_reply.started":"2024-12-21T14:32:37.524543Z","shell.execute_reply":"2024-12-21T14:32:38.106011Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/bengali_transliteration_model/tokenizer_config.json',\n '/kaggle/working/bengali_transliteration_model/special_tokens_map.json',\n '/kaggle/working/bengali_transliteration_model/spiece.model',\n '/kaggle/working/bengali_transliteration_model/added_tokens.json')"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:54:21.487268Z","iopub.execute_input":"2024-12-21T14:54:21.487600Z","iopub.status.idle":"2024-12-21T14:54:21.506160Z","shell.execute_reply.started":"2024-12-21T14:54:21.487573Z","shell.execute_reply":"2024-12-21T14:54:21.505098Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c84ac669eff497f9cdb79c72990d6d0"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from huggingface_hub import upload_folder\nmodel_dir = '/kaggle/working/bengali_transliteration_model'\nrepo_name = 'DJ2003/my-bengali-transliteration-model'\nupload_folder(\n    folder_path=model_dir,          \n    repo_id=repo_name,              \n    path_in_repo=''                 \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:55:32.741086Z","iopub.execute_input":"2024-12-21T14:55:32.741404Z","iopub.status.idle":"2024-12-21T14:55:42.896688Z","shell.execute_reply.started":"2024-12-21T14:55:32.741372Z","shell.execute_reply":"2024-12-21T14:55:42.895999Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288479925ded4db4b23437cc51567704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae65373bf2fc4b258c5256646c56ecde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e5bd0f356048169e82899a046be4fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410113590f484aa394839bf1454d8861"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/DJ2003/my-bengali-transliteration-model/commit/baeb28e4f4975c509c61eb46183603d1d8ae04bd', commit_message='Upload folder using huggingface_hub', commit_description='', oid='baeb28e4f4975c509c61eb46183603d1d8ae04bd', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"from transformers import MBartTokenizer\n\ntokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-50-many-to-one-mmt')\nprint(tokenizer.lang_code_to_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T15:10:00.313535Z","iopub.execute_input":"2024-12-21T15:10:00.313844Z","iopub.status.idle":"2024-12-21T15:10:01.796276Z","shell.execute_reply.started":"2024-12-21T15:10:00.313821Z","shell.execute_reply":"2024-12-21T15:10:01.795372Z"}},"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \nThe class this function is called from is 'MBartTokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"{'ar_AR': 250001, 'cs_CZ': 250002, 'de_DE': 250003, 'en_XX': 250004, 'es_XX': 250005, 'et_EE': 250006, 'fi_FI': 250007, 'fr_XX': 250008, 'gu_IN': 250009, 'hi_IN': 250010, 'it_IT': 250011, 'ja_XX': 250012, 'kk_KZ': 250013, 'ko_KR': 250014, 'lt_LT': 250015, 'lv_LV': 250016, 'my_MM': 250017, 'ne_NP': 250018, 'nl_XX': 250019, 'ro_RO': 250020, 'ru_RU': 250021, 'si_LK': 250022, 'tr_TR': 250023, 'vi_VN': 250024, 'zh_CN': 250025}\n","output_type":"stream"}],"execution_count":28}]}